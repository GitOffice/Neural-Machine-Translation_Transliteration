{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing fastai.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "French/English parallel texts from http://www.statmt.org/wmt15/translation-task.html .  It was created by Chris Callison-Burch, who crawled millions of web pages and then used *a set of simple heuristics to transform French URLs onto English URLs (i.e. replacing \"fr\" with \"en\" and about 40 other hand-written rules), and assume that these documents are translations of each other*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!ls data/translate/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each line contains a pair of sentences both in french and in english separated by a tabulation (TSV : Tab Separated Values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Va !\tGo.\r\n",
      "Cours !\tRun!\r\n",
      "Courez !\tRun!\r\n",
      "Ça alors !\tWow!\r\n",
      "Au feu !\tFire!\r\n"
     ]
    }
   ],
   "source": [
    "#!head -n 5 data/en-fr.txt\n",
    "!head -n 5 data/fr-en.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data should be found in the file en-fr.txt in the folder data/  \n",
    "data/tmp will contain temporary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('data')\n",
    "TMP_PATH = PATH/'tmp'\n",
    "TMP_PATH.mkdir(exist_ok=True)\n",
    "#fname = PATH/'en-fr.txt'\n",
    "fname = PATH/'fr-en.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the file with utf-8 encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = list(open(fname, encoding='utf-8'))\n",
    "#lines = lines[:50] # the first 50 pairs, comment out this line to train on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149861"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lines = lines[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = np.random.choice(lines, 100)\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"Vous n'êtes pas invitée.\\tYou aren't invited.\\n\", 'Il sauta une classe.\\tHe skipped a year.\\n',\n",
       "       \"On ne peut pas faire entendre raison à Tom.\\tYou can't reason with Tom.\\n\",\n",
       "       \"Je ne voulais pas que vous vous sentiez seul.\\tI didn't want you to feel you were alone.\\n\",\n",
       "       \"Ne laisse pas les enfants monopoliser la télévision.\\tDon't let the children monopolize the television.\\n\"],\n",
       "      dtype='<U637')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating english and french sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "en = []\n",
    "fr = []\n",
    "for line in lines:\n",
    "    #enSentence, frSentence = line.rstrip().split('\\t') # .rstrip() to remove the trailing \\n in each line\n",
    "    frSentence, enSentence = line.rstrip().split('\\t') # .rstrip() to remove the trailing \\n in each line\n",
    "    en.append(enSentence)\n",
    "    fr.append(frSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"You aren't invited.\", \"Vous n'êtes pas invitée.\"),\n",
       " ('He skipped a year.', 'Il sauta une classe.'),\n",
       " (\"You can't reason with Tom.\", 'On ne peut pas faire entendre raison à Tom.'),\n",
       " (\"I didn't want you to feel you were alone.\",\n",
       "  'Je ne voulais pas que vous vous sentiez seul.'),\n",
       " (\"Don't let the children monopolize the television.\",\n",
       "  'Ne laisse pas les enfants monopoliser la télévision.')]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(en[:5], fr[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the sentences as pickle files (.pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(en, (PATH/'en.pkl').open('wb'))\n",
    "pickle.dump(fr, (PATH/'fr.pkl').open('wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the sentences, reading them from pickle files  \n",
    "##### Add some logic before all the previous code, if you find the following files: {en,fr}.pkl, load them directly without doing it all over again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "en = pickle.load((PATH/'en.pkl').open('rb'))\n",
    "fr = pickle.load((PATH/'fr.pkl').open('rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(\"You aren't invited.\", \"Vous n'êtes pas invitée.\"),\n",
       "  ('He skipped a year.', 'Il sauta une classe.'),\n",
       "  (\"You can't reason with Tom.\",\n",
       "   'On ne peut pas faire entendre raison à Tom.'),\n",
       "  (\"I didn't want you to feel you were alone.\",\n",
       "   'Je ne voulais pas que vous vous sentiez seul.'),\n",
       "  (\"Don't let the children monopolize the television.\",\n",
       "   'Ne laisse pas les enfants monopoliser la télévision.')],\n",
       " 100)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(en[:5], fr[:5])), len(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing the english sentences, using fastai and spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tok = Tokenizer.proc_all_mp(partition_by_cores(en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['you', 'are', \"n't\", 'invited', '.'],\n",
       " ['he', 'skipped', 'a', 'year', '.'],\n",
       " ['you', 'ca', \"n't\", 'reason', 'with', 'tom', '.'],\n",
       " ['i', 'did', \"n't\", 'want', 'you', 'to', 'feel', 'you', 'were', 'alone', '.'],\n",
       " ['do',\n",
       "  \"n't\",\n",
       "  'let',\n",
       "  'the',\n",
       "  'children',\n",
       "  'monopolize',\n",
       "  'the',\n",
       "  'television',\n",
       "  '.']]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_tok[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#!python -m spacy download fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing the french sentences, using fastai and spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fr_tok = Tokenizer.proc_all_mp(partition_by_cores(fr), 'fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['vous', \"n'\", 'êtes', 'pas', 'invitée', '.'],\n",
       " ['il', 'sauta', 'une', 'classe', '.'],\n",
       " ['on', 'ne', 'peut', 'pas', 'faire', 'entendre', 'raison', 'à', 'tom', '.'],\n",
       " ['je', 'ne', 'voulais', 'pas', 'que', 'vous', 'vous', 'sentiez', 'seul', '.'],\n",
       " ['ne',\n",
       "  'laisse',\n",
       "  'pas',\n",
       "  'les',\n",
       "  'enfants',\n",
       "  'monopoliser',\n",
       "  'la',\n",
       "  'télévision',\n",
       "  '.']]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_tok[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['you', 'are', \"n't\", 'invited', '.'],\n",
       " ['vous', \"n'\", 'êtes', 'pas', 'invitée', '.'])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_tok[0], fr_tok[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.percentile([len(o) for o in en_tok], 90), np.percentile([len(o) for o in fr_tok], 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep = np.array([len(o)<30 for o in en_tok])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#en_tok = np.array(en_tok)[keep]\n",
    "#fr_tok = np.array(fr_tok)[keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the tokenized sentences as pickle files (.pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(en_tok, (PATH/'en_tok.pkl').open('wb'))\n",
    "pickle.dump(fr_tok, (PATH/'fr_tok.pkl').open('wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the tokenized sentences, reading them from pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tok = pickle.load((PATH/'en_tok.pkl').open('rb'))\n",
    "fr_tok = pickle.load((PATH/'fr_tok.pkl').open('rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will get as arguments the list of tokenized sentences and their language ID  \n",
    "and returns the sentences converted to IDs, and the mappings ID -> token and token -> ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def toks2ids(tok,pre):\n",
    "    \"\"\"\n",
    "        Convert tokens to IDs\n",
    "        tok is the list of tokenized sentences\n",
    "        pre is the name of the language, example: 'fr', 'en', ...\n",
    "    \"\"\"\n",
    "    freq = Counter(p for o in tok for p in o) # puts all tokens in Counter(.)\n",
    "    \n",
    "    # for a token, returns its index, int-to-string\n",
    "    itos = [o for o,c in freq.most_common(40000)] # 40K most common tokens, returns tuples (token, frequency)\n",
    "    itos.insert(0, '_bos_') # begining of sentence\n",
    "    itos.insert(1, '_pad_') # padding\n",
    "    itos.insert(2, '_eos_') # end of sentence\n",
    "    itos.insert(3, '_unk_') # unknown\n",
    "    # itos = [_bos_, _pad_, _eos_, _unk_, token1, token2, ...]\n",
    "    \n",
    "    # reverse mapping: string-to-int\n",
    "    # returns 3 '_unk_' if the token isn't found\n",
    "    stoi = collections.defaultdict(lambda: 3, {v:k for k,v in enumerate(itos)})\n",
    "    # stoi = {_bos_:0, _pad_:1, _eos_:2, _unk_:3, token1:4, token2:5, ...}\n",
    "    \n",
    "    # converts the tokenized sentences: replaces the token by its id using stoi\n",
    "    # notice that we've appended 2 (_eos_) at the end of each sentence\n",
    "    ids = np.array([([stoi[o] for o in p] + [2]) for p in tok])\n",
    "    \n",
    "    # saving the ids to a .npy file\n",
    "    np.save(TMP_PATH/f'{pre}_ids.npy', ids)\n",
    "    \n",
    "    # saving the itos mapping to a .pkl file\n",
    "    pickle.dump(itos, open(TMP_PATH/f'{pre}_itos.pkl', 'wb'))\n",
    "    \n",
    "    return ids, itos, stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_ids, en_itos, en_stoi = toks2ids(en_tok,'en')\n",
    "fr_ids, fr_itos, fr_stoi = toks2ids(fr_tok,'fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will load the mappings (saved from the previous function)  \n",
    "and returns the sentences converted to IDs, and the mappings ID -> token and token -> ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ids(pre):\n",
    "    \"\"\"\n",
    "        load saved IDs\n",
    "        pre is the name of the language, example: 'fr', 'en', ...\n",
    "    \"\"\"\n",
    "    ids = np.load(TMP_PATH/f'{pre}_ids.npy')\n",
    "    itos = pickle.load(open(TMP_PATH/f'{pre}_itos.pkl', 'rb'))\n",
    "    stoi = collections.defaultdict(lambda: 3, {v:k for k,v in enumerate(itos)})\n",
    "    return ids, itos, stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_ids, en_itos, en_stoi = load_ids('en')\n",
    "fr_ids, fr_itos, fr_stoi = load_ids('fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is how to use the ID -> token mapping to reconstruct the original tokenized sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['vous', \"n'\", 'êtes', 'pas', 'invitée', '.', '_eos_'], 368)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[fr_itos[o] for o in fr_ids[0]], len(fr_itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['you', 'are', \"n't\", 'invited', '.', '_eos_'], 304)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[en_itos[o] for o in en_ids[0]], len(en_itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([5, 14, 9, 95, 4, 2]), list([15, 96, 11, 97, 4, 2]), list([5, 98, 9, 99, 30, 17, 4, 2]),\n",
       "       list([6, 18, 9, 20, 5, 7, 100, 5, 52, 101, 4, 2]), list([12, 9, 53, 8, 102, 103, 8, 104, 4, 2])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_ids[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fasttext word vectors available from https://fasttext.cc/docs/en/english-vectors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install git+https://github.com/facebookresearch/fastText.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastText as ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the fastText library, you'll need to download [fasttext word vectors](https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md) for your language (download the 'bin plus text' ones)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings  en.pkl\tfr-en.txt  fr_tok.pkl  tmp\t    wiki.fr.pkl\r\n",
      "en-fr.txt   en_tok.pkl\tfr.pkl\t   models      wiki.en.pkl\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the embeddings (word vectors), the embeddings used here are trained on the text corpus of the bible  \n",
    "replace them with other embeddings (wikitext for example) to get better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#en_vecs = ft.load_model(str((PATH/'wiki.en.bin')))\n",
    "en_vecs = ft.load_model(str((PATH/'embeddings/en.bin')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fr_vecs = ft.load_model(str((PATH/'wiki.fr.bin')))\n",
    "fr_vecs = ft.load_model(str((PATH/'embeddings/fr.bin')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even, if the word isn't found in the embedding Fasttext, find and returns its unique embedding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.42811,  0.00472, -0.55592,  0.23108,  0.25022, -0.24158, -0.45753, -0.35123, -0.33665, -0.10807,\n",
       "        -0.08436, -0.25136, -0.56695, -0.84764, -0.1322 ,  0.14316, -0.48988, -0.327  , -0.20238,  0.10418,\n",
       "         0.64037, -0.52406,  0.37063,  0.08053, -0.22075, -0.3133 , -0.16693,  0.27541, -0.83423,  0.72061,\n",
       "         0.26193, -0.26927,  0.33067, -0.29437, -0.18973,  0.24391,  0.21215, -0.18755,  0.08848, -0.49965,\n",
       "         0.66024,  0.34113,  0.05566, -0.03431, -0.08241,  0.18887,  0.10322,  0.30522,  1.01105,  0.70856,\n",
       "        -0.02141, -0.09663, -0.51842,  0.40998,  0.36047,  0.09259,  0.12745,  0.1713 ,  0.67702,  0.51115,\n",
       "         0.59954,  0.11556,  0.1054 , -0.3921 , -0.13521,  0.14449, -0.54714, -0.13172, -0.06877,  0.21863,\n",
       "        -0.00522,  0.24992, -0.22379, -0.1001 ,  0.02345,  0.55127,  0.0052 , -0.47208,  0.02044, -0.11235,\n",
       "        -0.61071,  0.62704, -0.20642, -0.0206 ,  0.05281, -0.07086, -0.25795, -0.68857,  0.2554 , -0.37531,\n",
       "        -0.41069,  0.25751,  0.08411, -0.02867, -0.11942,  0.22636, -0.36852, -0.11474, -0.1757 , -0.28935],\n",
       "       dtype=float32), 100)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = en_vecs.get_word_vector('word')\n",
    "emb, len(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.07974,  0.08461,  0.35847,  0.18693,  0.0896 ,  0.41236,  0.03115,  0.32625,  0.28907,  0.14129,\n",
       "        -0.33977, -0.14277,  0.09088,  0.03029, -0.27774,  0.04753, -0.09452,  0.27132, -0.22255,  0.38899,\n",
       "        -0.35074, -0.20456, -0.2683 ,  0.32908,  0.01609, -0.23642, -0.14939,  0.16693,  0.00914, -0.2081 ,\n",
       "         0.52242, -0.14854,  0.08188,  0.51043, -0.37571, -0.14249, -0.16533, -0.23474,  0.20654, -0.31629,\n",
       "        -0.1563 , -0.19745, -0.48685,  0.28842, -0.22111, -0.12055, -0.10207,  0.16942,  0.41112,  0.10763,\n",
       "        -0.00341,  0.6312 , -0.06367, -0.29418, -0.09617, -0.12286,  0.02321,  0.5479 ,  0.12541,  0.66944,\n",
       "        -0.04868,  0.71005,  0.36709, -0.00205,  0.29375,  0.15777,  0.08099,  0.0686 ,  0.15842,  0.1739 ,\n",
       "        -0.32733,  0.24166, -0.19596,  0.30417, -0.26513, -0.22868, -0.31623, -0.30047, -0.15973, -0.61035,\n",
       "         0.49594, -0.15864,  0.234  ,  0.58409,  0.22567,  0.04142, -0.24763,  0.18914, -0.05537, -0.62398,\n",
       "         0.30289, -0.13245, -0.31832,  0.22952, -0.25613,  0.14468, -0.26194,  0.33744,  0.2319 ,  0.62623],\n",
       "       dtype=float32), 100)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = fr_vecs.get_word_vector('mot')\n",
    "emb, len(emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better way than using the following function is to use fr_vecs and en_vecs directly  \n",
    "Using a dictionary will trigger a KeyError if we want to get the embedding vector of a word which is not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_vecs(lang, ft_vecs):\n",
    "#    \"\"\"\n",
    "#        Creates a dictionary of {word: word vector} from the fasttext object ft_vecs\n",
    "#        and saving it into a pickle file .pkl\n",
    "#    \"\"\"\n",
    "#    # we can get an error if we want to get the word vector of a word which isn't in this dictionary !\n",
    "#    vecd = {w:ft_vecs.get_word_vector(w) for w in ft_vecs.get_words()}\n",
    "#    \n",
    "#    # saving the vector in a .pkl file\n",
    "#    pickle.dump(vecd, open(PATH/f'wiki.{lang}.pkl','wb'))\n",
    "#    return vecd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#en_vecd = get_vecs('en', en_vecs)\n",
    "#fr_vecd = get_vecs('fr', fr_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#en_vecd = pickle.load(open(PATH/'wiki.en.pkl','rb'))\n",
    "#fr_vecd = pickle.load(open(PATH/'wiki.fr.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ft_vecs = en_vecs\n",
    "#ft_words = ft_vecs.get_words(include_freq=True)\n",
    "#ft_words[0][:5], ft_words[1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ft_word_dict = {k:v for k,v in zip(*ft_words)}\n",
    "#ft_word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ft_words = sorted(ft_word_dict.keys(), key=lambda x: ft_word_dict[x])\n",
    "#ft_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(ft_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the following functions is a better idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFrWordVector(word):\n",
    "    return fr_vecs.get_word_vector(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEnWordVector(word):\n",
    "    return en_vecs.get_word_vector(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_en_vec = len(getEnWordVector(','))\n",
    "dim_fr_vec = len(getFrWordVector(','))\n",
    "dim_en_vec, dim_fr_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dim_en_vec = len(en_vecd[','] if ',' in en_vecd.keys() else en_vecs.get_word_vector(','))\n",
    "#dim_fr_vec = len(fr_vecd[','] if ',' in fr_vecd.keys() else fr_vecs.get_word_vector(','))\n",
    "#dim_en_vec, dim_fr_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#en_vecs_np = np.stack(list(en_vecd.values()))\n",
    "#en_vecs_np.mean(), en_vecs_np.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to know the mean and the variance of our word vectors  \n",
    "we can use the following function to do that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStatistics(fasttextVectors):\n",
    "    vecd = {w:fasttextVectors.get_word_vector(w) for w in fasttextVectors.get_words()}\n",
    "    vecs_np = np.stack(list(vecd.values()))\n",
    "    return vecs_np.mean(), vecs_np.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.027238548, 0.30283847)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getStatistics(fr_vecs) # (0.027238548, 0.30283847)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.018482028, 0.27999485)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getStatistics(en_vecs) # (0.018482028, 0.27999485)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10653, 8375)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fr_vecs.get_words()), len(en_vecs.get_words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both sets of vectors have rughly the same statistics, about 0 mean and 0.3 standard deviation.  \n",
    "If we multiply the vectors by 0.3, we'll get a new set of vectors (normalized) with mean = 0 and standard deviation = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truncating the data isn't a good idea at all ! we shouldn't alter the data, we have to use as it is !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enlen_90 = int(np.percentile([len(o) for o in en_ids], 99))\n",
    "#frlen_90 = int(np.percentile([len(o) for o in fr_ids], 97))\n",
    "#enlen_90, frlen_90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#en_ids_tr = np.array([o[:enlen_90] for o in en_ids])\n",
    "#fr_ids_tr = np.array([o[:frlen_90] for o in fr_ids])\n",
    "#en_ids_tr[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following class is a generic PyTorch dataset, used to prepare the data to be used by PyTorch and fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, x, y): self.x, self.y = x, y\n",
    "    def __getitem__(self, idx): return A(self.x[idx], self.y[idx])\n",
    "    #def __getitem__(self, idx): yield A(self.x[idx], self.y[idx])\n",
    "    def __len__(self): return len(self.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-Test split: 90% training / 10% testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86, 14)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train-test split\n",
    "#np.random.seed(42)\n",
    "trn_keep = np.random.rand(len(en_ids)) > 0.1\n",
    "en_trn, fr_trn = en_ids[trn_keep], fr_ids[trn_keep]\n",
    "en_val, fr_val = en_ids[~trn_keep], fr_ids[~trn_keep]\n",
    "len(en_trn),len(en_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_ds = Seq2SeqDataset(fr_trn, en_trn)\n",
    "val_ds = Seq2SeqDataset(fr_val, en_val)\n",
    "\n",
    "#trn_ds = Seq2SeqDataset(en_trn, fr_trn)\n",
    "#val_ds = Seq2SeqDataset(en_val, fr_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bs=125 # batch size\n",
    "bs=1 # batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SortishSampler** : Returns an iterator that traverses the data in randomly ordered batches that are approximately the same size. (used for training set)  \n",
    "**SortSampler** : Returns a sorted iterator (used for testing set)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_samp = SortishSampler(en_trn, key=lambda x: len(en_trn[x]), bs=bs)\n",
    "val_samp = SortSampler(en_val, key=lambda x: len(en_val[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "??ModelData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classifier** → padding in the beginning. Because we want that final token to represent the last word of the movie review.  \n",
    "**Decoder** → padding at the end. As you will see, it actually is going to work out a bit better to have the padding at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "??DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we don't have any data augmentation tasks, so we can set num_workers to 1\n",
    "# we have to transpose the tensors (why ?!!!)\n",
    "# we use 1 (_pad_) to pad the shortest tensor, so that all the tensors would have the same length\n",
    "#     a tensor must be rectangular\n",
    "# we want our padding to be at the end, not at the start, that's why pre_pad is set to False\n",
    "trn_dl = DataLoader(trn_ds, bs, transpose=True, transpose_y=True, num_workers=1, \n",
    "                    pad_idx=1, pre_pad=False, sampler=trn_samp)\n",
    "\n",
    "# why int(bs*1.6) ?!!!\n",
    "val_dl = DataLoader(val_ds, int(bs*1.6), transpose=True, transpose_y=True, num_workers=1, \n",
    "                    pad_idx=1, pre_pad=False, sampler=val_samp)\n",
    "\n",
    "# PATH is used to save temporary files, models, ...\n",
    "md = ModelData(PATH, trn_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 7, 18, 43,  6, 97,  4,  2]), array([ 5, 14,  9, 95,  4,  2])]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      " 366\n",
      "   5\n",
      "  15\n",
      " 367\n",
      "  55\n",
      "   2\n",
      "[torch.LongTensor of size 6x1]\n",
      ", \n",
      " 302\n",
      "  94\n",
      " 303\n",
      "   4\n",
      "   2\n",
      "[torch.LongTensor of size 5x1]\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(11, 10), (9, 7), (7, 8), (8, 10), (10, 9)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it = iter(trn_dl)\n",
    "\n",
    "#for i in it:\n",
    "#    print(i)\n",
    "\n",
    "print(next(it))\n",
    "\n",
    "\"\"\"\n",
    "The (1)s are the padding characters _pad_\n",
    "The (2)s at the end are the end-of-sentence tokens _eos_\n",
    "[\n",
    "   45     7    61    46    10   123   125    62    13    20\n",
    "    8    42     5     4    26     8     4     5    28    56\n",
    "   16     4     2     2     5    16     2     2     5     6\n",
    "    4     2    (1)   (1)    2     4    (1)   (1)    2     4\n",
    "    2    (1)   (1)   (1)   (1)    2    (1)   (1)   (1)    2\n",
    "[torch.LongTensor of size 5x10]\n",
    ", \n",
    "   23     6    11    10     6    23    10    11     7    24\n",
    "   22    17    12    16    56    22    12    12    31     5\n",
    "    4     4     4     4     4     4     4     4     4     2\n",
    "    2     2     2     2     2     2     2     2     2    (1)\n",
    "[torch.LongTensor of size 4x10]\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "its = [next(it) for i in range(5)]\n",
    "[(len(x),len(y)) for x,y in its]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "?nn.Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word vectors have a mean around 0 and a variance around 3, that's why we multiplied the vector by 3  \n",
    "The general rule is : word_vector x variance + mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def create_emb(vecs, itos, em_sz):\n",
    "#    emb = nn.Embedding(len(itos), em_sz, padding_idx=1) # 1: _pad_, dimension: (vocab size x embedding size)\n",
    "#    wgts = emb.weight.data\n",
    "#    miss = []\n",
    "#    for i,w in enumerate(itos):\n",
    "#        try: wgts[i] = torch.from_numpy(vecs[w]*3)\n",
    "#        except: miss.append(w)\n",
    "#    print(len(miss),miss[5:10])\n",
    "#    return emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a vector standard normal (have a mean of 0 and a standard deviation of 1)  \n",
    "\n",
    "$$Y = \\frac{X-\\mu}{\\sigma}$$  \n",
    "\n",
    "$\\mu$ is the mean of X, $\\sigma$ is its standard deviation.  \n",
    "We can create a function to do just that ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(vector, mean, standardDeviation):\n",
    "    \"\"\"\n",
    "        This function returns the 'normalized' version of vector\n",
    "        by subtracting the mean than dividing by the standard deviation\n",
    "        Hepler function to be used in the function `create_emb`\n",
    "    \"\"\"\n",
    "    return (vector - mean) / standardDeviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version uses the fasttext object firectly so no mises ! even if the word doesn't exist in the original embeddings, fasttext will generate its unique embedding vector. I don't want to miss any word ! whatever was its frequency !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ old implementation ------\n",
    "#def create_emb(fasttextVectors, itos, em_sz, mean=0, standardDeviation=1): # standardDeviation is 0.3 in our example\n",
    "#    emb = nn.Embedding(len(itos), em_sz, padding_idx=1) # 1: _pad_, dimension: (vocab size x embedding size)\n",
    "#    wgts = emb.weight.data\n",
    "#    #miss = []\n",
    "#    for i,w in enumerate(itos):\n",
    "#        #assert len(fasttextVectors.get_word_vector(w)) == em_sz\n",
    "#        #wgts[i] = torch.from_numpy(fasttextVectors.get_word_vector(w)*standardDeviation)\n",
    "#        wgts[i] = torch.from_numpy(normalize(fasttextVectors.get_word_vector(w), mean, standardDeviation))\n",
    "#        #try: wgts[i] = torch.from_numpy(fasttextVectors.get_word_vector(w)*standardDeviation)\n",
    "#        #except: miss.append(w)\n",
    "#    #print(len(miss))\n",
    "#    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_emb(en_vecs, en_itos, 100, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FasttextVectorsWrapper():\n",
    "    \"\"\"\n",
    "        Adapter for fasttext objects, used to normalize the embedding vectors\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, fasttextVectors, mean=0, standardDeviation=1):\n",
    "        self.fasttextVectors = fasttextVectors\n",
    "        self.mean = mean\n",
    "        self.standardDeviation = standardDeviation\n",
    "    \n",
    "    def get_word_vector(self, word):\n",
    "        return normalize(\n",
    "            self.fasttextVectors.get_word_vector(word),\n",
    "            self.mean,\n",
    "            self.standardDeviation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# getStatistics(fr_vecs) # (0.027238548, 0.30283847)\n",
    "fr_vecs = FasttextVectorsWrapper(fr_vecs, 0.027238548, 0.30283847)\n",
    "\n",
    "# getStatistics(en_vecs) # (0.018482028, 0.27999485)\n",
    "en_vecs = FasttextVectorsWrapper(en_vecs, 0.018482028, 0.27999485)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_vecs = FasttextVectorsWrapper(fr_vecs, 0.027238548, 0.30283847)\n",
    "en_vecs = FasttextVectorsWrapper(en_vecs, 0.018482028, 0.27999485)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb(fasttextVectorsWrapped, itos, em_sz):\n",
    "    emb = nn.Embedding(len(itos), em_sz, padding_idx=1) # 1: _pad_, dimension: (vocab size x embedding size)\n",
    "    wgts = emb.weight.data\n",
    "    for i,w in enumerate(itos):\n",
    "        wgts[i] = torch.from_numpy(fasttextVectorsWrapped.get_word_vector(w))\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(304, 100, padding_idx=1)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_emb(en_vecs, en_itos, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "nh, nl = 256, 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xxx_enc are encoder's variables, xxx_dec are decoder's variables  \n",
    "out_xxx is what's going to be at the output of the encoder / decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the GRU computations  \n",
    "$$\n",
    "        \\begin{array}{ll}\n",
    "        r_t = \\mathrm{sigmoid}(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\\\\n",
    "        z_t = \\mathrm{sigmoid}(W_{iz} x_t + b_{iz} + W_{hz} h_{(t-1)} + b_{hz}) \\\\\n",
    "        n_t = \\tanh(W_{in} x_t + b_{in} + r_t * (W_{hn} h_{(t-1)}+ b_{hn})) \\\\\n",
    "        h_t = (1 - z_t) * n_t + z_t * h_{(t-1)} \\\\\n",
    "        \\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "?nn.GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "?torch.stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a new tensor with a dimension of size one inserted at the specified position.\n",
    "?torch.unsqueeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    * The embedding layers are used to get the fasttext embedding vector from the token's index in the vocabulary\n",
    "    (we can also fine tune the embeddings, they will become learnable parameters, and train them end to end)\n",
    "    * emb_enc: Embeddings for the encoder\n",
    "    * emb_dec: Embeddings for the decoder\n",
    "    * we can use LSTMs instead of GRUs\n",
    "    * The embedding size of pretrained fasttext vectors is 300\n",
    "    * The embeddings can be learnable: they have the attribute weight which is a Variable containing the attribute\n",
    "        data which is a Tensor\n",
    "    * The embeddings are initially random, we go through our vocabulary, if we find the token, we replace the\n",
    "        random vector by its corresponding fasttext embedding, the random vectors have a standard deviation of1\n",
    "        but our fasttext vectors have a standard deviation of 0.3, that's why we've multiplied by 3\n",
    "    \"\"\"\n",
    "    def __init__(self, vecs_enc, itos_enc, em_sz_enc, vecs_dec, itos_dec, em_sz_dec, nh, out_sl, nl=2):\n",
    "        super().__init__()\n",
    "        self.nl, self.nh, self.out_sl = nl, nh, out_sl\n",
    "        self.emb_enc = create_emb(vecs_enc, itos_enc, em_sz_enc)\n",
    "        \n",
    "        # adding dropout prevent overfitting (add visualization of dropout ? in the presentation and report)\n",
    "        # analogy: died neurons in the brain !\n",
    "        self.emb_enc_drop = nn.Dropout(0.15)\n",
    "        \n",
    "        # the RNN cell (GRU, LSTM)\n",
    "        # the input to the RNN is the size of the embedding\n",
    "        # nh is the number of hidden units\n",
    "        # nl is the number of layers\n",
    "        # 0.25 dropout inside the RNN\n",
    "        self.gru_enc = nn.GRU(em_sz_enc, nh, num_layers=nl, dropout=0.25)\n",
    "        \n",
    "        # transform the hidden output (RNN output) into something to be fed to the decoder\n",
    "        # this is a simple transformation, so it should be a matrix multiplication without adding a bias\n",
    "        # this operation is the reverse of the embedding, and the embedding doesn’t have a bias. (jeremy's answer)\n",
    "        # that's why bias is set to False\n",
    "        self.out_enc = nn.Linear(nh, em_sz_dec, bias=False)\n",
    "        \n",
    "        self.emb_dec = create_emb(vecs_dec, itos_dec, em_sz_dec)\n",
    "        self.gru_dec = nn.GRU(em_sz_dec, em_sz_dec, num_layers=nl, dropout=0.1)\n",
    "        self.out_drop = nn.Dropout(0.35)\n",
    "        self.out = nn.Linear(em_sz_dec, len(itos_dec))\n",
    "        self.out.weight.data = self.emb_dec.weight.data\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        # sl: sentence length, bs: batch size\n",
    "        sl, bs = inp.size()\n",
    "        h = self.initHidden(bs)\n",
    "        \n",
    "        # putting the input into the embedding, then through the dropout\n",
    "        emb = self.emb_enc_drop(self.emb_enc(inp))\n",
    "        \n",
    "        # we put the initial hidden state (zeros) and the embedding to the RNN\n",
    "        # which spits out the final hidden state\n",
    "        enc_out, h = self.gru_enc(emb, h)\n",
    "        \n",
    "        # the final hidden state is passing through the linear layer so that we'll\n",
    "        # have something in the right size to feed to our decoder\n",
    "        h = self.out_enc(h)\n",
    "        \n",
    "        # input ---> embedding ---> hidden vector (h, which is the same as s in the figure shown in the course)\n",
    "        # the hidden vector hopefully will learn to contain all the information about what that sentence\n",
    "        # says and how it says it\n",
    "\n",
    "        # the previous translated word, first it is NULL (vector of zeros) --> _bos_ token\n",
    "        # decoder input, initially zeros (the _bos_ beginning of stream token is 0)\n",
    "        # long because the output should be the indices\n",
    "        dec_inp = V(torch.zeros(bs).long()) # bs is the batch size\n",
    "        \n",
    "        res = []\n",
    "        \n",
    "        # self.out_sl is the length of the largest english sentence, because we're translating into english\n",
    "        # it can't be possibly longer than that (at least in our corpus), it we use it on longer sentences\n",
    "        # this will fail, we can pass in a bigger value\n",
    "        # --> why we don't change it to a while loop (while the token isn't _eos_ or _pad_ ?)\n",
    "        for i in range(self.out_sl):\n",
    "            \n",
    "            # the basic idea is the same: we put the input through the embedding\n",
    "            # adding a unit axis / dimension in the specified position using .unsqueeze(0)\n",
    "            # this embedding use the target embedding vectors, in our example, the english word vectors\n",
    "            # because we're translating from french to english: fr --> en\n",
    "            emb = self.emb_dec(dec_inp).unsqueeze(0)\n",
    "            \n",
    "            # then through the RNN\n",
    "            outp, h = self.gru_dec(emb, h)\n",
    "            \n",
    "            # stick the output of the RNN through the dropout then through a linear layer in order to convert\n",
    "            # that into the correct size for the decoder embedding matrix (to be used in the next iteration\n",
    "            # to predict the next word)\n",
    "            outp = self.out(self.out_drop(outp[0]))\n",
    "            \n",
    "            # append the result to the results list (list of translated words)\n",
    "            # it doesn't have a fixed length ;)\n",
    "            res.append(outp) # we append the probabilities here not the word !\n",
    "            \n",
    "            # outp is a tensor whose length is the number of words in our english (target language) vocabulary\n",
    "            # and it contains the probability for every one of those words that it is that word\n",
    "            \n",
    "            # we have to know what word what to feed in to the next timestamp\n",
    "            \n",
    "            # outp.data.max(1) looks for the highest probability\n",
    "            # max(.) in PyTorch returns 2 things, the maximum value and the argmax\n",
    "            # (the index into the array at which we've found the maximum value)\n",
    "            # and we need the index, that's why there is a [1] after the max (to get the 2nd element)\n",
    "            \n",
    "            # now dec_inp is whatever the highest probability word was\n",
    "            dec_inp = V(outp.data.max(1)[1]) # contains the word index in our vocabulary\n",
    "            \n",
    "            # if it's a 1 (_pad_), that's mean we're done, we reached the end\n",
    "            # else, we continue\n",
    "            if (dec_inp==1).all(): break\n",
    "                \n",
    "            # we keep looping through either until we get to the largest length of a sentence or until\n",
    "            # everything in our mini batch is padding\n",
    "\n",
    "        # at the end, we're going to stack that list up into a single tensor and return it\n",
    "        # now, we can feed this tensor into a loss function\n",
    "        return torch.stack(res)\n",
    "    \n",
    "    # used to initialize the hidden state vector with zeros\n",
    "    def initHidden(self, bs): return V(torch.zeros(self.nl, bs, self.nh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Normally, a RNN works on a whole sequence at a time but we've got a for loop to go through each part of the sequence separately so we have to add a leading unit access to the start `unsqueeze(0)`  \n",
    "* Predicting what is the $4^{th}$ word of a sentence without knowing the $3^{rd}$ one is hard ! that's why we need to input the last translated (at time $t-1$) word at each time step to predict the next one (at time $t$)\n",
    "* Let's think about one part of the batch / one token, with start with a 0 / \\_bos\\_,  we look up that zero in our embedding matrix to find out what's the vector for the beginning of stream token is, we stick a unit axis on the front to say we have a single sequence, we stick this through our RNN, at first, it will get 2 things: the \\_bos\\_ token and the hidden state which is the output if the encoder and its task now is to figure out what is the $1^{st}$ word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       "[torch.LongTensor of size 5]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# suppose we have a batch_size equals to 5, this is how dec_inp would look like at the begining\n",
    "V(torch.zeros(5).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0  0  0  0  0\n",
       "[torch.LongTensor of size 1x5]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding a unit axis at the begining (notice how the size changed from 5 to 1x5)\n",
    "V(torch.zeros(5).long()).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.9586  0.0265  0.8499  0.9537  0.8167  0.6053  0.2144  0.9046  0.0376  0.5733\n",
       "[torch.FloatTensor of size 1x10]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for simplicity we'll supose that our vocabulary have 10 words\n",
    "nextWordProbabilities = torch.rand(1,10)\n",
    "nextWordProbabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0\n",
       "[torch.LongTensor of size 1]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nextWordProbabilities.max(1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,.,.) = \n",
       "  0.2579  0.7438  0.0966  0.6420\n",
       "  0.5827  0.1917  0.7428  0.2305\n",
       "  0.0241  0.0927  0.6222  0.9577\n",
       "\n",
       "(1 ,.,.) = \n",
       "  0.2562  0.8452  0.4747  0.4619\n",
       "  0.5867  0.7910  0.7250  0.4064\n",
       "  0.5269  0.1850  0.0392  0.2084\n",
       "[torch.FloatTensor of size 2x3x4]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(2,3,4)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       "  0.5827  0.7438  0.7428  0.9577\n",
       "  0.5867  0.8452  0.7250  0.4619\n",
       " [torch.FloatTensor of size 2x4], \n",
       "  1  0  1  2\n",
       "  1  0  1  0\n",
       " [torch.LongTensor of size 2x4])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.max(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "the loss function is categorical cross-entropy loss ( https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/ )  \n",
    "we've got a list of probabilities for each of our classes, the classes are all the words in our English vocab and we have a target which is the correct class, the correct word at this location.  \n",
    "* If the generated sequence length is shorter than the sequence length of the target, we need to add some padding. PyTorch padding function requires a tuple of 6 to pad a rank 3 tensor (sequence length, batch size, by number of words in the vocab). Each pair represents padding before and after that dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "?F.pad\n",
    "#    the dimensions that gets padded begins with the last dimension and moves forward.\n",
    "#    >>> t4d = torch.Tensor(3, 3, 4, 2)\n",
    "#    >>> p3d = (0, 1, 2, 1, 3, 3) # pad by (0, 1), (2, 1), and (3, 3)\n",
    "#    >>> out = F.pad(t4d, p3d, \"constant\", 0)\n",
    "#    >>> print(out.data.size())\n",
    "#    torch.Size([3, 9, 7, 3])\n",
    "#    (3, 3, 4, 2) --> (3, 3 + 3 + 3, 4 + 2 + 1, 2 + 0 + 1) --> (3, 9, 7, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.5208  0.7188  0.1717\n",
       " 0.6729  0.1863  0.1165\n",
       "[torch.FloatTensor of size 2x3]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(2, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.5208  0.7188  0.1717  0.0000  0.0000\n",
       " 0.0000  0.6729  0.1863  0.1165  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "[torch.FloatTensor of size 9x6]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.pad(x, (1, 2, 3, 4), 'constant', 0)\n",
    "# (2, 3) --> (2 + 3 + 4, 3 + 1 + 2) = (9, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.0000  0.5208  0.7188  0.1717  0.0000  0.0000\n",
       " 0.0000  0.6729  0.1863  0.1165  0.0000  0.0000\n",
       "[torch.FloatTensor of size 2x6]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.pad(x, (1, 2), 'constant', 0)\n",
    "# (2, 3) --> (2, 3 + 1 + 2) = (2, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.0000  0.0000  0.0000\n",
       " 0.5208  0.7188  0.1717\n",
       " 0.6729  0.1863  0.1165\n",
       " 0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000\n",
       "[torch.FloatTensor of size 5x3]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.pad(x, (0, 0, 1, 2), 'constant', 0)\n",
    "# (2, 3) --> (2 + 1 + 2, 3 + 0 + 0) = (5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.5727  0.8534  0.3840\n",
       " 0.2433  0.1825  0.9726\n",
       "[torch.FloatTensor of size 2x3]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(2, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.5727  0.8534  0.3840  0.2433  0.1825  0.9726\n",
       "[torch.FloatTensor of size 1x6]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.view(1,-1) # flattens out the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "?F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_loss(input, target):\n",
    "    sl, bs = target.size() # sequence_length x batch_size (target)\n",
    "    sl_in, bs_in, nc = input.size() # sequence_length x batch_size x vocab_size (input)\n",
    "    \n",
    "    # if the input is shorter than the target, add padding at the end (after the last dimension)\n",
    "    if sl > sl_in: input = F.pad(input, (0, 0, 0, 0, 0, sl-sl_in))\n",
    "    # (sl_in x bs x nc) --> (sl_in + sl - sl_in x bs x nc) = (sl x bs x nc)\n",
    "\n",
    "    input = input[:sl]\n",
    "    \n",
    "    # F.cross_entropy expects a rank 2 tensor, but we have sequence length by batch size,\n",
    "    # so let’s just flatten out. That is what view(-1, ...) does.\n",
    "    return F.cross_entropy(input.view(-1,nc), target.view(-1))#, ignore_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "??partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "?optim.Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between `.cuda()` and `to_gpu()` : `to_gpu` will not put to in the GPU if you do not have one. You can also set `fastai.core.USE_GPU` to `false` to force it to not use GPU that can be handy for debugging.  \n",
    "* We then need something that tells it how to handle learning rate groups so there is a thing called SingleModel that you can pass it to which treats the whole thing as a single learning rate group [1:09:40]. So this is the easiest way to turn a PyTorch module into a fastai model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#??SingleModel\n",
    "??BasicModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SingleModel` is used to turn a PyTorch module into a fastai module and `RNN_Learner` is used to turn a fastai module into a learner.  \n",
    "* We could just call Learner to turn that into a learner, but if we call `RNN_Learner`, it does add in `save_encoder` and `load_encoder` that can be handy sometimes. In this case, we really could have said `Learner` but `RNN_Learner` also works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`def __init__(self, vecs_enc, itos_enc, em_sz_enc, vecs_dec, itos_dec, em_sz_dec, nh, out_sl, nl=2):`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enlen_90 = int(np.percentile([len(o) for o in en_ids], 99))\n",
    "def getMaxLenSentence(ids):\n",
    "    return max([len(o) for o in ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#rnn = Seq2SeqRNN(fr_vecd, fr_itos, dim_fr_vec, en_vecd, en_itos, dim_en_vec, nh, enlen_90) # old implementation\n",
    "\n",
    "# length of the longest english sentence in our dataset !\n",
    "maxLenSentence = getMaxLenSentence(en_ids)\n",
    "#maxLenSentence = getMaxLenSentence(fr_ids)\n",
    "rnn = Seq2SeqRNN(fr_vecs, fr_itos, dim_fr_vec, en_vecs, en_itos, dim_en_vec, nh, maxLenSentence) # new implementation\n",
    "#rnn = Seq2SeqRNN(en_vecs, en_itos, dim_en_vec, fr_vecs, fr_itos, dim_fr_vec, nh, maxLenSentence) # new implementation\n",
    "\n",
    "# md is the model data object, containing the train and validation dataloaders\n",
    "learn = RNN_Learner(md, SingleModel(to_gpu(rnn)), opt_fn=opt_fn)\n",
    "\n",
    "# once we have a learner, we give it our loss function\n",
    "learn.crit = seq2seq_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test jupyter notebook's widgets\n",
    "#from ipywidgets import IntProgress\n",
    "#IntProgress(10,max=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fastai.core.USE_GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c14021ec09fa4c6d95c4078d8702384c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 66%|██████▋   | 57/86 [00:25<00:12,  2.28it/s, loss=25.6]"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa7e63bc198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#import pdb; pdb.set_trace()\n",
    "learn.lr_find()\n",
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "?torch.index_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=3e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can safely ignore the error 'Set changed size during iteration'  \n",
    "http://forums.fast.ai/t/runtimeerror-set-changed-size-during-iteration/7565"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f5530b319364e6aa3fa4ae5d0f61cfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=12), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                            \n",
      "    0      5.570353   5.410881  \n",
      "    1      5.371669   4.897469                            \n",
      "    2      5.023494   5.038763                            \n",
      "    3      4.77653    4.759721                            \n",
      "    4      4.551561   4.860833                            \n",
      "    5      4.397697   4.742536                            \n",
      "    6      4.088735   4.747053                            \n",
      "    7      3.842484   4.598875                            \n",
      "    8      3.65064    4.619178                            \n",
      "    9      3.40026    4.525529                            \n",
      "    10     3.144407   4.471087                            \n",
      "    11     3.020039   4.393691                            \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([4.39369])]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(lr, 1, cycle_len=12, use_clr=(20,10))\n",
    "\n",
    "# using try..except to ignore the error\n",
    "# RuntimeError: Set changed size during iteration\n",
    "#try:\n",
    "#    learn.fit(lr, 1, cycle_len=10, use_clr=(20,10))\n",
    "#except RuntimeError:\n",
    "#    print('RuntimeError detected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('initial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('initial')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tom et mary acceptèrent de travailler ensemble sur le projet . _eos_\n",
      "tom and mary agreed to work together on the project . _eos_\n",
      "tom tom tom tom to to _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "je ne pense pas que tu aies fait ceci par toi-même . _eos_\n",
      "i do n't think you did this by yourself . _eos_\n",
      "i i have you you to to to . _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "peut-être que tom peut t' aider à trouver un travail . _eos_\n",
      "perhaps tom can help you find a job . _eos_\n",
      "you you a a to to to _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "j' aime prendre un petit déjeuner tardif . _eos_\n",
      "i like to eat a late breakfast . _eos_\n",
      "a a a to to to _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "pourquoi ne discutez - vous pas avec lui ? _eos_\n",
      "why do n't you talk to him ? _eos_\n",
      "do you you you a . _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "ne le ramassez pas . _eos_\n",
      "do n't pick it up . _eos_\n",
      "do the the the . . _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "lui et moi sommes instituteurs . _eos_\n",
      "he and i are teachers . _eos_\n",
      "he he your . _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "as - tu apporté une arme ? _eos_\n",
      "did you bring a weapon ? _eos_\n",
      "how you you you _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "ils n' ont pas écouté . _eos_\n",
      "they did n't listen . _eos_\n",
      "you are . _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "vous connaissez la rengaine . _eos_\n",
      "you know the drill . _eos_\n",
      "you you a . _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "es - tu bientôt prêt ? _eos_\n",
      "are you almost ready ? _eos_\n",
      "why you you you _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "nous petit-déjeunons . _eos_\n",
      "we are having breakfast . _eos_\n",
      "you are is . _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "ferme les yeux . _eos_\n",
      "shut your eyes . _eos_\n",
      "the the the the . _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "attention aux imitations . _eos_\n",
      "beware of imitations . _eos_\n",
      "you are is . _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#x,y = next(iter(val_dl))\n",
    "#probs = learn.model(V(x))\n",
    "#preds = to_np(probs.max(2)[1])\n",
    "\n",
    "for x, y in iter(val_dl):\n",
    "    #print(x)\n",
    "    for i in range(x.shape[1]):\n",
    "        # fr -> en\n",
    "        print(' '.join([fr_itos[o] for o in x[:,i] if o != 1]))\n",
    "        print(' '.join([en_itos[o] for o in y[:,i] if o != 1]))\n",
    "        \n",
    "        probs = learn.model(V(x))\n",
    "        preds = to_np(probs.max(2)[1])\n",
    "        print(' '.join([en_itos[o] for o in preds[:,i] if o!=1]))\n",
    "        print()\n",
    "        \n",
    "        # en -> fr\n",
    "        #print(' '.join([en_itos[o] for o in x[:,i] if o != 1]))\n",
    "        #print(' '.join([fr_itos[o] for o in y[:,i] if o != 1]))\n",
    "        \n",
    "        #probs = learn.model(V(x))\n",
    "        #preds = to_np(probs.max(2)[1])\n",
    "        #print(' '.join([fr_itos[o] for o in preds[:,i] if o!=1]))\n",
    "        #print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Bidir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we added `bidirectional=True`, the Linear layer now needs number of hidden times 2 ( i.e. `nh*2` ) to reflect the fact that we have that second direction in our hidden state. Also in initHidden it’s now `self.nl*2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Seq2SeqRNN_Bidir(nn.Module):\n",
    "    def __init__(self, vecs_enc, itos_enc, em_sz_enc, vecs_dec, itos_dec, em_sz_dec, nh, out_sl, nl=2):\n",
    "        super().__init__()\n",
    "        self.nl, self.nh, self.out_sl = nl, nh, out_sl\n",
    "        self.emb_enc = create_emb(vecs_enc, itos_enc, em_sz_enc)\n",
    "        \n",
    "        # Bidirectional encoder, adds a 2nd RNN in the opposite direction\n",
    "        self.gru_enc = nn.GRU(em_sz_enc, nh, num_layers=nl, dropout=0.25, bidirectional=True)\n",
    "        \n",
    "        # doubling the number of input neurones in the linear layer\n",
    "        self.out_enc = nn.Linear(nh*2, em_sz_dec, bias=False)\n",
    "        \n",
    "        self.drop_enc = nn.Dropout(0.05)\n",
    "        \n",
    "        self.emb_dec = create_emb(vecs_dec, itos_dec, em_sz_dec)\n",
    "        self.gru_dec = nn.GRU(em_sz_dec, em_sz_dec, num_layers=nl, dropout=0.1)\n",
    "        \n",
    "        self.emb_enc_drop = nn.Dropout(0.15)\n",
    "        self.out_drop = nn.Dropout(0.35)\n",
    "        \n",
    "        self.out = nn.Linear(em_sz_dec, len(itos_dec))\n",
    "        self.out.weight.data = self.emb_dec.weight.data\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        sl,bs = inp.size()\n",
    "        h = self.initHidden(bs)\n",
    "        emb = self.emb_enc_drop(self.emb_enc(inp))\n",
    "        \n",
    "        enc_out, h = self.gru_enc(emb, h)\n",
    "        h = h.view(2,2,bs,-1).permute(0,2,1,3).contiguous().view(2,bs,-1)\n",
    "        h = self.out_enc(self.drop_enc(h))\n",
    "\n",
    "        dec_inp = V(torch.zeros(bs).long())\n",
    "        res = []\n",
    "        for i in range(self.out_sl):\n",
    "            emb = self.emb_dec(dec_inp).unsqueeze(0)\n",
    "            outp, h = self.gru_dec(emb, h)\n",
    "            outp = self.out(self.out_drop(outp[0]))\n",
    "            res.append(outp)\n",
    "            dec_inp = V(outp.data.max(1)[1])\n",
    "            if (dec_inp==1).all(): break\n",
    "        return torch.stack(res)\n",
    "    \n",
    "    # self.nl*2 because bidirectional is set to True\n",
    "    # one hidden state vector for each RNN\n",
    "    def initHidden(self, bs): return V(torch.zeros(self.nl*2, bs, self.nh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Why do you need to set a range to the loop? [1:20:58] Because when we start training, everything is random so `if (dec_inp==1).all(): break` will probably never be true. Later on, it will pretty much always break out eventually but basically we are going to go forever. It’s really important to remember when you are designing an architecture that when you start, the model knows nothing about anything. So you want to make sure if it’s going to do something at least it’s vaguely sensible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#rnn = Seq2SeqRNN_Bidir(fr_vecd, fr_itos, dim_fr_vec, en_vecd, en_itos, dim_en_vec, nh, enlen_90)\n",
    "\n",
    "maxLenSentence = getMaxLenSentence(fr_ids)\n",
    "rnn = Seq2SeqRNN_Bidir(fr_vecs, fr_itos, dim_fr_vec, en_vecs, en_itos, dim_en_vec, nh, maxLenSentence) # new implementation\n",
    "learn = RNN_Learner(md, SingleModel(to_gpu(rnn)), opt_fn=opt_fn)\n",
    "learn.crit = seq2seq_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "382fc35636f746458205a5041e4d667b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=12), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                            \n",
      "    0      5.863043   5.075959  \n",
      "    1      6.044207   5.02854                             \n",
      "    2      5.313451   4.963759                            \n",
      "    3      4.857013   4.650207                            \n",
      "    4      4.390748   4.654443                            \n",
      "    5      4.26524    4.730703                            \n",
      "    6      4.057316   4.539191                            \n",
      "    7      3.739889   4.551198                            \n",
      "    8      3.499628   4.310367                            \n",
      "    9      3.266146   4.364414                            \n",
      "    10     3.021368   4.386462                            \n",
      "    11     2.739096   4.28379                             \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([4.28379])]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(lr, 1, cycle_len=12, use_clr=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.save('bidir')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test - Bidir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tom et mary acceptèrent de travailler ensemble sur le projet . _eos_\n",
      "tom and mary agreed to work together on the project . _eos_\n",
      "tom tom too let with with . _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "je ne pense pas que tu aies fait ceci par toi-même . _eos_\n",
      "i do n't think you did this by yourself . _eos_\n",
      "i i do you to to to _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "peut-être que tom peut t' aider à trouver un travail . _eos_\n",
      "perhaps tom can help you find a job . _eos_\n",
      "please please please to you to to to _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "j' aime prendre un petit déjeuner tardif . _eos_\n",
      "i like to eat a late breakfast . _eos_\n",
      "i a to to to to _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "pourquoi ne discutez - vous pas avec lui ? _eos_\n",
      "why do n't you talk to him ? _eos_\n",
      "why do you you you _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "ne le ramassez pas . _eos_\n",
      "do n't pick it up . _eos_\n",
      "do do . . _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "lui et moi sommes instituteurs . _eos_\n",
      "he and i are teachers . _eos_\n",
      "he the . _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "as - tu apporté une arme ? _eos_\n",
      "did you bring a weapon ? _eos_\n",
      "you you you my about _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "ils n' ont pas écouté . _eos_\n",
      "they did n't listen . _eos_\n",
      "he the . . _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "vous connaissez la rengaine . _eos_\n",
      "you know the drill . _eos_\n",
      "you are the _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "es - tu bientôt prêt ? _eos_\n",
      "are you almost ready ? _eos_\n",
      "are you you mention ? _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "nous petit-déjeunons . _eos_\n",
      "we are having breakfast . _eos_\n",
      "we are _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "ferme les yeux . _eos_\n",
      "shut your eyes . _eos_\n",
      "the the . _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "attention aux imitations . _eos_\n",
      "beware of imitations . _eos_\n",
      "the the is _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn.load('bidir')\n",
    "x,y = next(iter(val_dl))\n",
    "probs = learn.model(V(x))\n",
    "preds = to_np(probs.max(2)[1])\n",
    "\n",
    "#print(x.shape)\n",
    "\n",
    "#for i in range(180,190):\n",
    "#for i in range(1):\n",
    "#    print(' '.join([fr_itos[o] for o in x[:,i] if o != 1]))\n",
    "#    print(' '.join([en_itos[o] for o in y[:,i] if o != 1]))\n",
    "#    print(' '.join([en_itos[o] for o in preds[:,i] if o!=1]))\n",
    "#    print()\n",
    "    \n",
    "for x, y in iter(val_dl):\n",
    "    #print(x)\n",
    "    for i in range(x.shape[1]):\n",
    "        # fr -> en\n",
    "        print(' '.join([fr_itos[o] for o in x[:,i] if o != 1]))\n",
    "        print(' '.join([en_itos[o] for o in y[:,i] if o != 1]))\n",
    "        \n",
    "        probs = learn.model(V(x))\n",
    "        preds = to_np(probs.max(2)[1])\n",
    "        print(' '.join([en_itos[o] for o in preds[:,i] if o!=1]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teacher forcing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s talk about teacher forcing. When a model starts learning, it knows nothing about nothing. So when the model starts learning, it is not going to spit out “Er” at the first step, it is going to spit out some random meaningless word because it doesn’t know anything about German or about English or about the idea of language. And it is going to feed it to the next process as an input and be totally unhelpful. That means, early learning is going to be very difficult because it is feeding in an input that is stupid into a model that knows nothing and somehow it’s going to get better. So it is not asking too much eventually it gets there, but it’s definitely not as helpful as we can be. So what if instead of feeding in the thing I predicted just now, what if we instead we feed in the actual correct word was meant to be. We can’t do that at inference time because by definition we don’t know the correct word - it has to translate it. We can’t require the correct translation in order to do translation.  \n",
    "![Seq2seq model][logo]\n",
    "[logo]: https://cdn-images-1.medium.com/max/800/1*DU776SGr1rhYeU7ilIKX9w.png \"Seq2seq model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch make it easier to implement teacher forcing not like other static graphs frameworks like Tensorflow or Keras.  \n",
    "Below, we're writing our own version of Stepper, instead of re-writting the whole training loop, this class will be responsible for decreasing pr_force after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class Seq2SeqStepper(Stepper):\n",
    "#    def step(self, xs, y, epoch):\n",
    "#        self.m.pr_force = (10-epoch)*0.1 if epoch<10 else 0\n",
    "#        xtra = []\n",
    "#        output = self.m(*xs, y)\n",
    "#        if isinstance(output,tuple): output,*xtra = output\n",
    "#        self.opt.zero_grad()\n",
    "#        loss = raw_loss = self.crit(output, y)\n",
    "#        if self.reg_fn: loss = self.reg_fn(output, xtra, raw_loss)\n",
    "#        loss.backward()\n",
    "#        if self.clip:   # Gradient clipping\n",
    "#            nn.utils.clip_grad_norm(trainable_params_(self.m), self.clip)\n",
    "#        self.opt.step()\n",
    "#        return raw_loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqStepper(Stepper):\n",
    "    def step(self, xs, y, epoch):\n",
    "        # the following line gradually decreases pr_force after each epoch (for the first 10 epochs)\n",
    "        # after 10 epochs pr_force will be 0\n",
    "        self.m.pr_force = (10-epoch)*0.1 if epoch<10 else 0\n",
    "        return super().step(xs, y, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the start of training, let’s set pr_force really high so that nearly always it gets the actual correct previous word and so it has a useful input. Then as we trained a bit more, let’s decrease pr_force so that by the end pr_force is zero and it has to learn properly which is fine because it is now actually feeding in sensible inputs most of the time anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqRNN_TeacherForcing(nn.Module):\n",
    "    def __init__(self, vecs_enc, itos_enc, em_sz_enc, vecs_dec, itos_dec, em_sz_dec, nh, out_sl, nl=2):\n",
    "        super().__init__()\n",
    "        self.emb_enc = create_emb(vecs_enc, itos_enc, em_sz_enc)\n",
    "        self.nl,self.nh,self.out_sl = nl,nh,out_sl\n",
    "        self.gru_enc = nn.GRU(em_sz_enc, nh, num_layers=nl, dropout=0.25)\n",
    "        self.out_enc = nn.Linear(nh, em_sz_dec, bias=False)\n",
    "        self.emb_dec = create_emb(vecs_dec, itos_dec, em_sz_dec)\n",
    "        self.gru_dec = nn.GRU(em_sz_dec, em_sz_dec, num_layers=nl, dropout=0.1)\n",
    "        self.emb_enc_drop = nn.Dropout(0.15)\n",
    "        self.out_drop = nn.Dropout(0.35)\n",
    "        self.out = nn.Linear(em_sz_dec, len(itos_dec))\n",
    "        self.out.weight.data = self.emb_dec.weight.data\n",
    "        self.pr_force = 1.\n",
    "        \n",
    "    def forward(self, inp, y=None):\n",
    "        sl,bs = inp.size()\n",
    "        h = self.initHidden(bs)\n",
    "        emb = self.emb_enc_drop(self.emb_enc(inp))\n",
    "        enc_out, h = self.gru_enc(emb, h)\n",
    "        h = self.out_enc(h)\n",
    "\n",
    "        dec_inp = V(torch.zeros(bs).long())\n",
    "        res = []\n",
    "        for i in range(self.out_sl):\n",
    "            emb = self.emb_dec(dec_inp).unsqueeze(0)\n",
    "            outp, h = self.gru_dec(emb, h)\n",
    "            outp = self.out(self.out_drop(outp[0]))\n",
    "            res.append(outp)\n",
    "            dec_inp = V(outp.data.max(1)[1])\n",
    "            if (dec_inp==1).all(): break\n",
    "\n",
    "            # pr_force: probability of forcing\n",
    "            # if a random number if less than this probability, ...\n",
    "            if (y is not None) and (random.random()<self.pr_force):\n",
    "                \n",
    "                # if it's longer than the target sentence, we stop\n",
    "                if i>=len(y): break\n",
    "                \n",
    "                # ... we're going to replace the decoder input with the actual correct word\n",
    "                dec_inp = y[i]\n",
    "                \n",
    "        return torch.stack(res)\n",
    "    \n",
    "    def initHidden(self, bs): return V(torch.zeros(self.nl, bs, self.nh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rnn = Seq2SeqRNN_TeacherForcing(fr_vecd, fr_itos, dim_fr_vec, en_vecd, en_itos, dim_en_vec, nh, enlen_90)\n",
    "\n",
    "maxLenSentence = getMaxLenSentence(fr_ids)\n",
    "rnn = Seq2SeqRNN_TeacherForcing(fr_vecs, fr_itos, dim_fr_vec, en_vecs, en_itos, dim_en_vec, nh, maxLenSentence) # new implementation\n",
    "learn = RNN_Learner(md, SingleModel(to_gpu(rnn)), opt_fn=opt_fn)\n",
    "learn.crit = seq2seq_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d33afe1e04a4d24b3678aa3b7eb8685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=12), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                            \n",
      "    0      5.592833   5.026546  \n",
      "    1      5.415341   5.207315                            \n",
      "    2      5.00852    4.857776                            \n",
      "    3      4.723488   4.760334                            \n",
      "    4      4.55963    4.600816                            \n",
      "    5      4.318899   4.587016                            \n",
      "    6      4.071668   4.615957                            \n",
      "    7      3.780182   4.620639                            \n",
      "    8      3.640652   4.609586                            \n",
      "    9      3.489344   4.552939                            \n",
      "    10     3.226872   4.538654                            \n",
      "    11     3.059119   4.524024                            \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([4.52402])]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we have to pass in our custom stepper class\n",
    "learn.fit(lr, 1, cycle_len=12, use_clr=(20,10), stepper=Seq2SeqStepper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('forcing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tom et mary acceptèrent de travailler ensemble sur le projet . _eos_\n",
      "tom and mary agreed to work together on the project . _eos_\n",
      "tom is is a to to _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "je ne pense pas que tu aies fait ceci par toi-même . _eos_\n",
      "i do n't think you did this by yourself . _eos_\n",
      "i i have you to to to . _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "peut-être que tom peut t' aider à trouver un travail . _eos_\n",
      "perhaps tom can help you find a job . _eos_\n",
      "you a a a to to to _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "j' aime prendre un petit déjeuner tardif . _eos_\n",
      "i like to eat a late breakfast . _eos_\n",
      "look a a a with . _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "pourquoi ne discutez - vous pas avec lui ? _eos_\n",
      "why do n't you talk to him ? _eos_\n",
      "do do you take take . . _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "ne le ramassez pas . _eos_\n",
      "do n't pick it up . _eos_\n",
      "do do the the the . . _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "lui et moi sommes instituteurs . _eos_\n",
      "he and i are teachers . _eos_\n",
      "he is a the . _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "as - tu apporté une arme ? _eos_\n",
      "did you bring a weapon ? _eos_\n",
      "how you you a _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "ils n' ont pas écouté . _eos_\n",
      "they did n't listen . _eos_\n",
      "do do . . _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "vous connaissez la rengaine . _eos_\n",
      "you know the drill . _eos_\n",
      "you you is . _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "es - tu bientôt prêt ? _eos_\n",
      "are you almost ready ? _eos_\n",
      "why you you you a _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "nous petit-déjeunons . _eos_\n",
      "we are having breakfast . _eos_\n",
      "we are . . _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "ferme les yeux . _eos_\n",
      "shut your eyes . _eos_\n",
      "the the is the . _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "attention aux imitations . _eos_\n",
      "beware of imitations . _eos_\n",
      "the the the _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x, y in iter(val_dl):\n",
    "    for i in range(x.shape[1]):\n",
    "        # fr -> en\n",
    "        print(' '.join([fr_itos[o] for o in x[:,i] if o != 1]))\n",
    "        print(' '.join([en_itos[o] for o in y[:,i] if o != 1]))\n",
    "        \n",
    "        probs = learn.model(V(x))\n",
    "        preds = to_np(probs.max(2)[1])\n",
    "        print(' '.join([en_itos[o] for o in preds[:,i] if o!=1]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attentional model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea of attention is this — expecting the entirety of the sentence to be summarized into this single hidden vector is asking a lot. It has to know what was said, how it was said, and everything necessary to create the sentence in German. The idea of attention is basically maybe we are asking too much. Particularly because we could use this form of model (below) where we output every step of the loop to not just have a hidden state at the end but to have a hidden state after every single word. Why not try and use that information? It’s already there but so far we’ve just been throwing it away. Not only that but bi-directional, we got two vectors of state every step that we can use. How can we do this?  \n",
    "\n",
    "![multiple output RNN][logo]\n",
    "[logo]: https://cdn-images-1.medium.com/max/800/1*CX45skUFZZO6uHsR8IndzA.png \"multiple output RNN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s say we are translating a word “liebte” right now [1:32:34]. Which of previous 5 pieces of state do we want? We clearly want “love” because it is the word. How about “zu”? We probably need “eat” and “to” and loved” to make sure we have gotten the tense right and know that I actually need this part of the verb and so forth. So depending on which bit we are translating, we would need one or more bits of these various hidden states. In fact, we probably want some weighting of them. In other words, for these five pieces of hidden state, we want a weighted average [1:33:47]. We want it weighted by something that can figure out which bits of the sentence is the most important right now. How do we figure out something like which bits of the sentence are important right now? We create a neural net and we train the neural net to figure it out. When do we train that neural net? End to end. So let’s now train two neural nets [1:34:18]. Well, we’ve already got a bunch — RNN encoder, RNN decoder, a couple of linear layers, what the heck, let’s add another neural net into the mix. This neural net is going to spit out a weight for every one of these states and we will take the weighted average at every step, and it’s just another set of parameters that we learn all at the same time. So that is called “attention”.\n",
    "\n",
    "![attention mechanism][logo]\n",
    "[logo]: https://cdn-images-1.medium.com/max/800/1*fkL30nxS54fKVmyC2jtMrw.png \"attention mechanism\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With attention, most of the code is identical. The one major difference is this line: `Xa = (a.unsqueeze(2) * enc_out).sum(0)` . We are going to take a weighted average and the way we are going to do the weighted average is we create a little neural net which we are going to see here:  \n",
    "```python\n",
    "w2h = self.l2(h[-1])\n",
    "u = F.tanh(w1e + w2h)\n",
    "a = F.softmax(u @ self.V, 0)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_t(*sz): return torch.randn(sz)/math.sqrt(sz[0]) # random tensor\n",
    "def rand_p(*sz): return nn.Parameter(rand_t(*sz)) # wrapped up in a Parameter\n",
    "\n",
    "# Parameter is just a PyTorch variable, it's like identical to a variable that it just tells PyTorch\n",
    "# I want you to learn the weights for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqAttnRNN(nn.Module):\n",
    "    def __init__(self, vecs_enc, itos_enc, em_sz_enc, vecs_dec, itos_dec, em_sz_dec, nh, out_sl, nl=2):\n",
    "        super().__init__()\n",
    "        self.emb_enc = create_emb(vecs_enc, itos_enc, em_sz_enc)\n",
    "        self.nl,self.nh,self.out_sl = nl,nh,out_sl\n",
    "        self.gru_enc = nn.GRU(em_sz_enc, nh, num_layers=nl, dropout=0.25)\n",
    "        self.out_enc = nn.Linear(nh, em_sz_dec, bias=False)\n",
    "        self.emb_dec = create_emb(vecs_dec, itos_dec, em_sz_dec)\n",
    "        self.gru_dec = nn.GRU(em_sz_dec, em_sz_dec, num_layers=nl, dropout=0.1)\n",
    "        self.emb_enc_drop = nn.Dropout(0.15)\n",
    "        self.out_drop = nn.Dropout(0.35)\n",
    "        self.out = nn.Linear(em_sz_dec*2, len(itos_dec))\n",
    "        self.out.weight.data = self.emb_dec.weight.data\n",
    "\n",
    "        # instead of a linear layer, we can even just grab a random matrix if we don't care about bias\n",
    "        self.W1 = rand_p(nh, em_sz_dec)\n",
    "        self.l2 = nn.Linear(em_sz_dec, em_sz_dec)\n",
    "        self.l3 = nn.Linear(em_sz_dec+nh, em_sz_dec)\n",
    "        self.V = rand_p(em_sz_dec)\n",
    "\n",
    "    def forward(self, inp, y=None, ret_attn=False):\n",
    "        sl,bs = inp.size()\n",
    "        h = self.initHidden(bs)\n",
    "        emb = self.emb_enc_drop(self.emb_enc(inp))\n",
    "        \n",
    "        # enc_out is the whole tensor of all of the encoder outputs\n",
    "        # the RNN spits out 2 things:\n",
    "        # list of states after each timestep, and the state at the last timestep\n",
    "        # using Python we can return multiple objects unlike other languages\n",
    "        enc_out, h = self.gru_enc(emb, h)\n",
    "        \n",
    "        # we used the state at the last time step to create the input state for our decoder\n",
    "        h = self.out_enc(h)\n",
    "\n",
    "        dec_inp = V(torch.zeros(bs).long())\n",
    "        res, attns = [], []\n",
    "        w1e = enc_out @ self.W1\n",
    "        for i in range(self.out_sl):\n",
    "            \n",
    "            \"\"\"\n",
    "                We use softmax because the nice thing about softmax is that we want to ensure all\n",
    "                of the weights that we are using add up to 1 and we also expect that one of those\n",
    "                weights should probably be higher than the other ones [1:36:38]. Softmax gives us\n",
    "                the guarantee that they add up to 1 and because it has e^ in it, it tends to\n",
    "                encourage one of the weights to be higher than the other ones.\n",
    "            \"\"\"\n",
    "            \n",
    "            # creating a neural net\n",
    "            \n",
    "            # what's the information we use to decide what word / words we should focus on next\n",
    "            # the only information we have to go on is what the decoder's hidden state is now\n",
    "            \n",
    "            # h[-1] is the last layer's hidden state which is the current hidden state of the decoder\n",
    "            # we stick it into a linear layer\n",
    "            w2h = self.l2(h[-1])\n",
    "            \n",
    "            # then stick it into a non-linear activation\n",
    "            u = F.tanh(w1e + w2h)\n",
    "            \n",
    "            # applying softmax to ensure that all of the weights add up to 1\n",
    "            # and we also kind of expect that one of those weights should\n",
    "            # probably be quite a bit higher than the other ones\n",
    "            \n",
    "            # then we're going to do a matrix multiplication\n",
    "            # (In Python, A @ B is the matrix product, A * B the element-wise product)\n",
    "            # stick it to a softmax\n",
    "            # self.V doesn't have a bias, so, u @ self.V is just a matrix multiplication\n",
    "            a = F.softmax(u @ self.V, 0)\n",
    "            \n",
    "            # linear layer --> nonlinear activation --> matrix multiply = neural net with one hidden layer\n",
    "            \n",
    "            attns.append(a)\n",
    "            \n",
    "            # and we're going to use the softmax output to weight the encoder's outputs\n",
    "            # instead of using only the last encoder output, we're using all of them weighted by the neural net\n",
    "            # we created (attention weights)\n",
    "            Xa = (a.unsqueeze(2) * enc_out).sum(0) # weighted average\n",
    "            \n",
    "            # the idea is not to use the last hidden state h only but all of the encoder states enc_out\n",
    "            # and multiply them by the output of the neural net (which predicts the attention weights)\n",
    "            \n",
    "            # given that the things in this little neural net are learnable weights, hopefully, it's\n",
    "            # going to learn to weight those encoder outputs, those encoder hidden states by something useful\n",
    "            \n",
    "            emb = self.emb_dec(dec_inp)\n",
    "            wgt_enc = self.l3(torch.cat([emb, Xa], 1))\n",
    "            \n",
    "            outp, h = self.gru_dec(wgt_enc.unsqueeze(0), h)\n",
    "            outp = self.out(self.out_drop(outp[0]))\n",
    "            res.append(outp)\n",
    "            dec_inp = V(outp.data.max(1)[1])\n",
    "            if (dec_inp==1).all(): break\n",
    "            \n",
    "            # Here, we have teacher forcing, but it's not bi-directional\n",
    "            if (y is not None) and (random.random()<self.pr_force):\n",
    "                if i>=len(y): break\n",
    "                dec_inp = y[i]\n",
    "\n",
    "        res = torch.stack(res)\n",
    "        if ret_attn: res = res, torch.stack(attns)\n",
    "        return res\n",
    "\n",
    "    def initHidden(self, bs): return V(torch.zeros(self.nl, bs, self.nh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the loss values are logs, so `e^` to these values is quite a significant change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rnn = Seq2SeqAttnRNN(fr_vecd, fr_itos, dim_fr_vec, en_vecd, en_itos, dim_en_vec, nh, enlen_90)\n",
    "\n",
    "maxLenSentence = getMaxLenSentence(en_ids)\n",
    "rnn = Seq2SeqAttnRNN(fr_vecs, fr_itos, dim_fr_vec, en_vecs, en_itos, dim_en_vec, nh, maxLenSentence) # new implementation\n",
    "learn = RNN_Learner(md, SingleModel(to_gpu(rnn)), opt_fn=opt_fn)\n",
    "learn.crit = seq2seq_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr=2e-3\n",
    "lr=5e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d92b5fa477840f8acd36f6f31caac32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=15), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                            \n",
      "    0      19.528716  15.789285 \n",
      "    1      27.617228  19.811899                           \n",
      "    2      25.820983  17.501958                           \n",
      "    3      23.395736  15.900296                           \n",
      "    4      21.87846   17.650807                           \n",
      "    5      18.729571  15.140794                           \n",
      "    6      26.027504  17.623428                           \n",
      "    7      18.98835   13.993108                           \n",
      "    8      15.549684  14.099075                           \n",
      "    9      14.069646  13.516583                           \n",
      "    10     12.506038  13.08138                            \n",
      "    11     11.151686  12.206781                           \n",
      "    12     10.347207  11.840073                           \n",
      "    13     9.598738   10.900506                           \n",
      "    14     8.896447   10.704287                           \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([10.70429])]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(lr, 1, cycle_len=15, use_clr=(20,10), stepper=Seq2SeqStepper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('attn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('attn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x,y = next(iter(val_dl))\n",
    "probs, attns = learn.model(V(x), ret_attn=True)\n",
    "preds = to_np(probs.max(2)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tom et mary acceptèrent de travailler ensemble sur le projet . _eos_\n",
      "tom and mary agreed to work together on the project . _eos_\n",
      "_eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "je ne pense pas que tu aies fait ceci par toi-même . _eos_\n",
      "i do n't think you did this by yourself . _eos_\n",
      "_eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "peut-être que tom peut t' aider à trouver un travail . _eos_\n",
      "perhaps tom can help you find a job . _eos_\n",
      "_eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "j' aime prendre un petit déjeuner tardif . _eos_\n",
      "i like to eat a late breakfast . _eos_\n",
      "_eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "pourquoi ne discutez - vous pas avec lui ? _eos_\n",
      "why do n't you talk to him ? _eos_\n",
      "_eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "ne le ramassez pas . _eos_\n",
      "do n't pick it up . _eos_\n",
      "_eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "lui et moi sommes instituteurs . _eos_\n",
      "he and i are teachers . _eos_\n",
      "_eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "as - tu apporté une arme ? _eos_\n",
      "did you bring a weapon ? _eos_\n",
      "_eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "ils n' ont pas écouté . _eos_\n",
      "they did n't listen . _eos_\n",
      "_eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "vous connaissez la rengaine . _eos_\n",
      "you know the drill . _eos_\n",
      "_eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "es - tu bientôt prêt ? _eos_\n",
      "are you almost ready ? _eos_\n",
      "_eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "nous petit-déjeunons . _eos_\n",
      "we are having breakfast . _eos_\n",
      "_eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "ferme les yeux . _eos_\n",
      "shut your eyes . _eos_\n",
      "_eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "attention aux imitations . _eos_\n",
      "beware of imitations . _eos_\n",
      "_eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x,y in iter(val_dl):\n",
    "    #print(x)\n",
    "    for i in range(x.shape[1]):\n",
    "        print(' '.join([fr_itos[o] for o in x[:,i] if o != 1]))\n",
    "        print(' '.join([en_itos[o] for o in y[:,i] if o != 1]))\n",
    "        print(' '.join([en_itos[o] for o in preds[:,i] if o!=1]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#attn = to_np(attns[...,180])\n",
    "attn = to_np(attns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa7e27d6fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(15, 10))\n",
    "for i,ax in enumerate(axes.flat):\n",
    "    ax.plot(attn[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention aux imitations . _eos_\n",
      "beware of imitations . _eos_\n",
      "_eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#for i in range(180,190):\n",
    "for i in range(x.shape[1]):\n",
    "    print(' '.join([fr_itos[o] for o in x[:,i] if o != 1]))\n",
    "    print(' '.join([en_itos[o] for o in y[:,i] if o != 1]))\n",
    "    print(' '.join([en_itos[o] for o in preds[:,i] if o!=1]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad. It’s still not perfect but quite a few of them are correct and again considering that we are asking it to learn about the very idea of language for two different languages and how to translate them between the two, and grammar, and vocabulary, and we only have 50,000 sentences and a lot of the words only appear once, I would say this is actually pretty amazing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End-to-end learning means you throw in everything you can into one loss function and the gradients of all the different parameters point in a direction that says “hey, you know if you had put more weight over there, it would have been better.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqRNN_All(nn.Module):\n",
    "    def __init__(self, vecs_enc, itos_enc, em_sz_enc, vecs_dec, itos_dec, em_sz_dec, nh, out_sl, nl=2):\n",
    "        super().__init__()\n",
    "        self.emb_enc = create_emb(vecs_enc, itos_enc, em_sz_enc)\n",
    "        self.nl,self.nh,self.out_sl = nl,nh,out_sl\n",
    "        self.gru_enc = nn.GRU(em_sz_enc, nh, num_layers=nl, dropout=0.25, bidirectional=True)\n",
    "        self.out_enc = nn.Linear(nh*2, em_sz_dec, bias=False)\n",
    "        self.drop_enc = nn.Dropout(0.25)\n",
    "        self.emb_dec = create_emb(vecs_dec, itos_dec, em_sz_dec)\n",
    "        self.gru_dec = nn.GRU(em_sz_dec, em_sz_dec, num_layers=nl, dropout=0.1)\n",
    "        self.emb_enc_drop = nn.Dropout(0.15)\n",
    "        self.out_drop = nn.Dropout(0.35)\n",
    "        self.out = nn.Linear(em_sz_dec, len(itos_dec))\n",
    "        self.out.weight.data = self.emb_dec.weight.data\n",
    "\n",
    "        self.W1 = rand_p(nh*2, em_sz_dec)\n",
    "        self.l2 = nn.Linear(em_sz_dec, em_sz_dec)\n",
    "        self.l3 = nn.Linear(em_sz_dec+nh*2, em_sz_dec)\n",
    "        self.V = rand_p(em_sz_dec)\n",
    "\n",
    "    def forward(self, inp, y=None):\n",
    "        sl,bs = inp.size()\n",
    "        h = self.initHidden(bs)\n",
    "        emb = self.emb_enc_drop(self.emb_enc(inp))\n",
    "        enc_out, h = self.gru_enc(emb, h)\n",
    "        h = h.view(2,2,bs,-1).permute(0,2,1,3).contiguous().view(2,bs,-1)\n",
    "        h = self.out_enc(self.drop_enc(h))\n",
    "\n",
    "        dec_inp = V(torch.zeros(bs).long())\n",
    "        res,attns = [],[]\n",
    "        w1e = enc_out @ self.W1\n",
    "        for i in range(self.out_sl):\n",
    "            w2h = self.l2(h[-1])\n",
    "            u = F.tanh(w1e + w2h)\n",
    "            a = F.softmax(u @ self.V, 0)\n",
    "            attns.append(a)\n",
    "            Xa = (a.unsqueeze(2) * enc_out).sum(0)\n",
    "            emb = self.emb_dec(dec_inp)\n",
    "            wgt_enc = self.l3(torch.cat([emb, Xa], 1))\n",
    "            \n",
    "            outp, h = self.gru_dec(wgt_enc.unsqueeze(0), h)\n",
    "            outp = self.out(self.out_drop(outp[0]))\n",
    "            res.append(outp)\n",
    "            dec_inp = V(outp.data.max(1)[1])\n",
    "            if (dec_inp==1).all(): break\n",
    "            if (y is not None) and (random.random()<self.pr_force):\n",
    "                if i>=len(y): break\n",
    "                dec_inp = y[i]\n",
    "        return torch.stack(res)\n",
    "\n",
    "    def initHidden(self, bs): return V(torch.zeros(self.nl*2, bs, self.nh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rnn = Seq2SeqRNN_All(fr_vecd, fr_itos, dim_fr_vec, en_vecd, en_itos, dim_en_vec, nh, enlen_90)\n",
    "maxLenSentence = getMaxLenSentence(en_ids)\n",
    "rnn = Seq2SeqRNN_All(fr_vecs, fr_itos, dim_fr_vec, en_vecs, en_itos, dim_en_vec, nh, maxLenSentence) # new implementation\n",
    "learn = RNN_Learner(md, SingleModel(to_gpu(rnn)), opt_fn=opt_fn)\n",
    "learn.crit = seq2seq_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67508e7069224470bdb6e8c2ca8057dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/86 [00:00<00:45,  1.85it/s, loss=31.1] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-62:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/stoufa/anaconda2/envs/fastai-cpu/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/stoufa/anaconda2/envs/fastai-cpu/lib/python3.6/site-packages/tqdm/_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/home/stoufa/anaconda2/envs/fastai-cpu/lib/python3.6/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                            \n",
      "    0      20.747583  23.754635 \n",
      "    1      21.734942  26.819055                           \n",
      "    2      41.705909  47.69762                            \n",
      "    3      40.685617  38.07177                            \n",
      "    4      42.198275  42.670547                           \n",
      "    5      43.92175   46.869793                           \n",
      "    6      44.144599  40.665845                           \n",
      "    7      42.901063  41.291865                           \n",
      "    8      43.685156  45.55401                            \n",
      "    9      42.153237  42.806808                           \n",
      "    10     37.173446  41.450775                           \n",
      "    11     37.949422  48.568005                           \n",
      "    12     37.752102  43.290563                           \n",
      "    13     36.914385  43.623336                           \n",
      "    14     33.63517   46.592167                           \n",
      "    15     35.546144  43.953029                           \n",
      "    16     32.720374  37.828381                           \n",
      "    17     33.256522  40.697285                           \n",
      "    18     30.983582  40.30966                            \n",
      "    19     30.595356  107.24651                           \n",
      "    20     34.206446  37.297674                           \n",
      "    21     31.585528  41.584774                           \n",
      "    22     30.299307  43.537001                           \n",
      "    23     27.570532  39.152183                           \n",
      "    24     25.199503  39.364039                           \n",
      "    25     24.797909  37.257355                           \n",
      "    26     22.583459  35.168086                           \n",
      "    27     22.137086  35.657256                           \n",
      "    28     21.640692  35.982632                           \n",
      "    29     20.137479  38.087865                           \n",
      "    30     19.184818  36.009848                           \n",
      "    31     18.626572  39.912701                           \n",
      "    32     18.23016   36.124416                           \n",
      "    33     17.13001   32.518638                           \n",
      "    34     15.242015  35.973507                           \n",
      "    35     14.976231  32.902606                           \n",
      "    36     14.360104  32.343511                           \n",
      "    37     13.958003  34.768097                           \n",
      "    38     13.380836  31.830265                           \n",
      "    39     12.713442  30.682205                           \n",
      "    40     12.148648  32.478058                           \n",
      "    41     11.485868  34.681602                           \n",
      "    42     11.020604  32.069301                           \n",
      "    43     10.618244  29.313803                           \n",
      "    44     10.274727  30.803117                           \n",
      "    45     9.804017   30.989262                           \n",
      "    46     9.739702   31.437856                           \n",
      "    47     9.25061    32.13971                            \n",
      "    48     8.98911    30.877722                           \n",
      "    49     8.422854   30.778272                           \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([30.77827])]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#learn.fit(lr, 1, cycle_len=15, use_clr=(20,10), stepper=Seq2SeqStepper)\n",
    "learn.fit(lr, 1, cycle_len=50, use_clr=(20,10), stepper=Seq2SeqStepper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tom et mary acceptèrent de travailler ensemble sur le projet . _eos_\n",
      "tom and mary agreed to work together on the project . _eos_\n",
      "_eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "je ne pense pas que tu aies fait ceci par toi-même . _eos_\n",
      "i do n't think you did this by yourself . _eos_\n",
      "_eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "peut-être que tom peut t' aider à trouver un travail . _eos_\n",
      "perhaps tom can help you find a job . _eos_\n",
      "_eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "j' aime prendre un petit déjeuner tardif . _eos_\n",
      "i like to eat a late breakfast . _eos_\n",
      "_eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "pourquoi ne discutez - vous pas avec lui ? _eos_\n",
      "why do n't you talk to him ? _eos_\n",
      "_eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "ne le ramassez pas . _eos_\n",
      "do n't pick it up . _eos_\n",
      ". . . . . . . . . . . . . . . . .\n",
      "\n",
      "lui et moi sommes instituteurs . _eos_\n",
      "he and i are teachers . _eos_\n",
      "_eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "as - tu apporté une arme ? _eos_\n",
      "did you bring a weapon ? _eos_\n",
      "? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?\n",
      "\n",
      "ils n' ont pas écouté . _eos_\n",
      "they did n't listen . _eos_\n",
      "_eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "vous connaissez la rengaine . _eos_\n",
      "you know the drill . _eos_\n",
      "the the the the the the the the the the the the the the the the the\n",
      "\n",
      "es - tu bientôt prêt ? _eos_\n",
      "are you almost ready ? _eos_\n",
      "_eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "nous petit-déjeunons . _eos_\n",
      "we are having breakfast . _eos_\n",
      "_eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "ferme les yeux . _eos_\n",
      "shut your eyes . _eos_\n",
      "_eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n",
      "attention aux imitations . _eos_\n",
      "beware of imitations . _eos_\n",
      "_eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_ _eos_\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x,y in iter(val_dl):\n",
    "    #for i in range(180,190):\n",
    "    for i in range(x.shape[1]):\n",
    "        print(' '.join([fr_itos[o] for o in x[:,i] if o != 1]))\n",
    "        print(' '.join([en_itos[o] for o in y[:,i] if o != 1]))\n",
    "        \n",
    "        probs = learn.model(V(x))\n",
    "        preds = to_np(probs.max(2)[1])\n",
    "        print(' '.join([en_itos[o] for o in preds[:,i] if o!=1]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fastai-cpu)",
   "language": "python",
   "name": "fastai-cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "253px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
